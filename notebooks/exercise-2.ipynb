{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Creating a Zarr Data Cube From a Timeseries\n",
    "\n",
    "We will now create a timeseries data cube over a single tile. To do this, we will read each tile into an xarray dataset, add a `year` dimension to the tile, and then \"stack\" them together along the `year` dimension. This will yield a 3 dimensional cube (year, x, y) of LULC data.\n",
    "\n",
    "We will employ the following xarray methods:\n",
    "- [`expand_dims`](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.expand_dims.html) adds a new dimension to the data array. In this case, we are adding a new dimension called \"year\" with a single value of either 2000, 2005, 2010, 2015, or 2020.\n",
    "- [`concat`](https://docs.xarray.dev/en/stable/generated/xarray.concat.html) concatenates multiple data arrays along a specified dimension. In this case, we are concatenating the data arrays along the \"year\" dimension. This is useful for creating a time series dataset from multiple time steps.\n",
    "\n",
    "Let's first consider a **** approach to this strategy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: get rid of chunking discussion since this should be small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEWARE: RUN ME AT YOUR (AND YOUR KERNEL'S) OWN RISK\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "\n",
    "file_name = \"40N_120W\"  # Feel free to change this to any of the other files in the dataset\n",
    "years = [2000, 2005, 2010, 2015, 2020]\n",
    "data_arrays = []\n",
    "\n",
    "for year in years:\n",
    "    url = f\"https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/v2/{year}/{file_name}.tif\"\n",
    "    # TODO: slice data here\n",
    "    da = rioxarray.open_rasterio(url, masked=True)\n",
    "    # NOTE: expand_dims will read the data in \n",
    "    da = da.expand_dims({\"year\": [year]})  # Add a 'year' dimension\n",
    "    data_arrays.append(da)\n",
    "\n",
    "# Concatenate the data arrays along the 'year' dimension\n",
    "combined = xr.concat(data_arrays, dim=\"year\") . # skip expand_dims \n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you attempted to run the above cell, you probably sat idly for several minutes before your kernel gave up. We are attempting to load a large amount of data here, so we will need to optimize a bit. That is where chunking comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "\n",
    "Data chunking in xarray (with Dask) is a way to break up large datasets into smaller, manageable pieces (\"chunks\") that can be processed lazily and in parallel. Itâ€™s essential when working with out-of-core data â€” data too big to fit into memory.\n",
    "\n",
    "Before we call `expand_dims`, which will load our data in, we need to chunk our xarray data. This leads to the age-old question: **How should I chunk my data?**\n",
    "\n",
    "While we could rely on `chunks=\"auto\"` to determine optimal chunks for us, let's do some actual math (I know, scary! ðŸ˜±)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math\n",
    "\n",
    "- Each tile is `(y: 40,000, x: 40,000)`, so there are `40,000 x 40,000 = 1.6 billion values with dtype=float32`\n",
    "- Each `float32` is 4 bytes, so the whole array is `1.6e9 x 4 bytes = 6.4 GB`\n",
    "\n",
    "We want to keep **chunk size between ~50MB to 200MB** for efficiency and to optimize for **access patterns** (ie processing entire rows vs entire tiles)\n",
    "\n",
    "Let's target ~100 MB chunks. Each `float32`=bytes, so:\n",
    "```\n",
    "chunk_size = (chunk_y, chunk_x)\n",
    "chunk_memory = chunk_y * chunk_x * 4 bytes\n",
    "```\n",
    "\n",
    "**Option 1: Chunk by tiles (e.g. 1000 x 1000)**\n",
    "```\n",
    "chunks = {\"y\": 1000, \"x\": 1000}\n",
    "memory_per_chunk = 1000 * 1000 * 4 = 4 MB\n",
    "```\n",
    "Too small â€” leads to **400** chunks per axis = **160,000 chunks total** ðŸ˜± (overhead!)\n",
    "\n",
    "**Option 2: Bigger tiles (e.g. 4000 x 4000)**\n",
    "```\n",
    "chunks = {\"y\": 4000, \"x\": 4000}\n",
    "memory_per_chunk = 4000 * 4000 * 4 = 64 MB\n",
    "```\n",
    "This results in 10 `y` chunks and 10 `x` chunks, so `100 total chunks`. This strikes a nice balance between chunk size and number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the above code again, but with 4000 x 4000 chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icechunk\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "\n",
    "file_name = \"40N_120W\"  # Feel free to change this to any of the other files in the dataset\n",
    "years = [2000, 2005, 2010, 2015]\n",
    "data_arrays = []\n",
    "\n",
    "for year in years:\n",
    "    url = f\"https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/v2/{year}/{file_name}.tif\"\n",
    "    da = rioxarray.open_rasterio(url, chunks={\"x\": 4000, \"y\": 4000})\n",
    "    da = da.rename({\"band\": \"lulc\"})\n",
    "    da = da.squeeze(\"lulc\")\n",
    "    # NOTE: expand_dims will read the data in \n",
    "    da = da.expand_dims(dim={\"year\": [year]})  # Add a 'year' dimension\n",
    "    data_arrays.append(da)\n",
    "\n",
    "# Concatenate the data arrays along the 'year' dimension\n",
    "combined = xr.concat(data_arrays, dim=\"year\")\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? So much faster! Let's discuss the data model here for a moment: we now have a 3D array of the shape `(year: 5, y: 40000, x: 40000)`. We have essentially stacked 5 years worth of 2D (x,y) array data into a data cube. TODO\n",
    "\n",
    "Now, let's do some analysis with our cube!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do stuff with combined!\n",
    "# Replot this data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: segway into Icechunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em-workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
