{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: *Soemthing catchy here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: [COG/TIF/GEOTIFF?] to Zarr with a single tile\n",
    "\n",
    "In this exercise, we will load in a single GeoTIFF into xarray using [rioxarray](https://corteva.github.io/rioxarray/html/modules.html) and show how to navigate the Xarray repr. We will then do some quick visualizations of the tile and save out the Xarray dataset to Zarr. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's read in a single GLAD LULC tile from the year 2000 from Google Cloud. The data can also be downloaded to local files [here](https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/v2/download.html). We will use rioxarray's [`open_rasterio`](https://corteva.github.io/rioxarray/html/rioxarray.html#rioxarray-open-rasterio) for this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "\n",
    "year = 2000  # Feel free to change this to 2005, 2010, 2015, or 2020\n",
    "file_name = \"50N_120W\"  # Feel free to change this to any of the other files in the dataset\n",
    "\n",
    "url = f\"https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/v2/{year}/{file_name}.tif\"\n",
    "\n",
    "da = rioxarray.open_rasterio(url, masked=True)\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine the data structure....\n",
    "\n",
    "TODO: Tom to fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note here that we did not actually read in all of the tile data; we actually only read in the metadata, which is why this was so quick! We will actually have to load the data in for operations that require direct data access like plotting and writing to Zarr. These will require some additional optimizations for these large tiles. \n",
    "\n",
    "For this first part of the tutorial, we will subset this tile to expedite the first few exercises. We will discuss optimizations when we get to building the global Zarr dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of the data\n",
    "x_slice=slice(-112.5, -111.5)  # None for whole tile\n",
    "y_slice=slice(41, 40.5)  # None for whole tile\n",
    "da_sample = da.sel(x=x_slice, y=y_slice)\n",
    "da_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find that for a lot of Level 3 geospatial datasets, the data is stored in a single band (often named \"band\" -- very original!) as it is here. Let's rename this band to \"lulc\" just to be a bit more explicit. \n",
    "\n",
    "We will also remove the `lulc` dim. Since it only has one value, it doesn't hold additional information along that axis, so removing it will simplify the array shape from a 3D array to a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_sample = da_sample.rename({\"band\": \"lulc\"})\n",
    "da_sample = da_sample.squeeze(\"lulc\")\n",
    "da_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Visualization is essential for geospatial data. How can we know that our data was correctly loaded into xarray without actually looking at it? Below are a few different approaches to plotting xarray data in a notebook. \n",
    "\n",
    "**Cloud vs. Local Latencies**\n",
    "\n",
    "Note that the data must be loaded in before it can be plotted. Loading data from the cloud has higher latency, and thus loading data in from a cloud source vs. from your local machine can cause a large disparity in runtime.\n",
    "\n",
    "**Visualizing in QGIS**\n",
    "\n",
    "QGIS natively supports TIFF and GeoTIFFS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into memory from the cloud\n",
    "# This may take awhile depending on your internet connection, the size of the file, and whether it is local or in cloud storage\n",
    "# This is slow because we are loading non-cloud optimized data\n",
    "# TODO: is this cloud optimized data?\n",
    "da_sample = da_sample.load()\n",
    "da_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leafmap\n",
    "\n",
    "[leafmap](https://leafmap.org/) is good for plotting xarray data because it combines the mapping power of Leaflet (via `ipyleaflet` or `folium`) with convenient tools for handling raster and vector geospatial data, including xarray. It can automatically convert xarray DataArrays into interactive map layers, supporting time sliders, colorbars, and basemaps — making it especially useful for visualizing geospatial timeseries or remote sensing data with minimal setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "\n",
    "def plot_leafmap(data_to_plot):\n",
    "    m = leafmap.Map(center=(40, -100), zoom=11)\n",
    "    m.add_raster(data_to_plot, colormap=\"tab20\", layer_name=\"LULC\")\n",
    "    m.add(\"inspector\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_leafmap(da_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hvPlot\n",
    "\n",
    "[hvPlot](https://hvplot.holoviz.org/) is great for large xarray datasets because it integrates well with xarray, supports Dask for lazy evaluation, and leverages Datashader to efficiently render millions of points without performance loss. It also enables interactive, zoomable plots with minimal code, making it ideal for exploring complex geospatial or time-series data.\n",
    "\n",
    "We discourage the use of dask-backed xarray dataset in this plotting example because **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray  # needed for hvplot\n",
    "import hvplot.pandas  # needed for tile sources\n",
    "import holoviews as hv\n",
    "from holoviews.element.tiles import EsriImagery  # or other tile source\n",
    "\n",
    "hv.extension('bokeh')\n",
    "\n",
    "def plot_hvplot(data_to_plot):\n",
    "    # rasterize=True will enable datashading for large datasets and will downsample the data based on the aggregation method\n",
    "    img = data_to_plot.hvplot.image(x='x', y='y', cmap='viridis', aggregator=\"first\", rasterize=True, frame_width=500, dynamic=True, geo=True)\n",
    "    return EsriImagery() * img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hvplot(da_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data to Zarr\n",
    "\n",
    "Before we move on from this single data tile, let's write our subset of data to Zarr using xarray's [`to_zarr`](https://docs.xarray.dev/en/latest/generated/xarray.Dataset.to_zarr.html) method.\n",
    "\n",
    "- `store`:\n",
    "- `group`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = \"\"\n",
    "group = \"\"\n",
    "da.to_zarr(store=store, group=group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: discussion of the Zarr data model using written out files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily read this dataset back into Xarray with [`open_zarr`](https://docs.xarray.dev/en/stable/generated/xarray.open_zarr.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_zarr(store=store, group=group)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Creating a Zarr Data Cube From a Timeseries\n",
    "\n",
    "We will now create a timeseries data cube over a single tile. To do this, we will read each tile into an xarray dataset, add a `year` dimension to the tile, and then \"stack\" them together along the `year` dimension. This will yield a 3 dimensional cube `(year, x, y)` of LULC data.\n",
    "\n",
    "We will first use a naive approach to illustrate this general flow with a very small sample of data. We will then work up to using more advanced approaches like Icechunk for version control and virtualization for ****.\n",
    "\n",
    "But, let's start with a straightforward example. We will employ Xarray's [`concat`](https://docs.xarray.dev/en/stable/generated/xarray.concat.html) method which concatenates multiple data arrays along a specified dimension. Here we add a new dimension called \"year\" with a single value of either 2000, 2005, 2010, 2015, or 2020. We will then concatenate the data arrays along the \"year\" dimension to create a \"stack\" of data. This method is useful for creating a time series dataset from multiple time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import xarray as xr\n",
    "\n",
    "file_name = \"50N_120W\"  # Feel free to change this to any of the other files in the dataset\n",
    "years = [2000, 2005, 2010, 2015, 2020]\n",
    "x_slice = slice(-112.5, -111.5)\n",
    "y_slice=slice(41, 40.5)\n",
    "data_arrays = []\n",
    "\n",
    "for year in years:\n",
    "    url = f\"https://storage.googleapis.com/earthenginepartners-hansen/GLCLU2000-2020/v2/{year}/{file_name}.tif\"\n",
    "    da = rioxarray.open_rasterio(url)\n",
    "    da = da.rename({\"band\": \"lulc\"})\n",
    "    da = da.squeeze(\"lulc\")\n",
    "    # Subset the data to the area of interest\n",
    "    da = da.sel(x=x_slice, y=y_slice)\n",
    "    data_arrays.append(da)\n",
    "\n",
    "# Concatenate the data arrays along the 'year' dimension\n",
    "# NOTE: this call reads all the data into memory and may take a while for large datasets\n",
    "combined = xr.concat(data_arrays, dim=xr.DataArray(years, dims=\"year\"))\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just read in 5x the amount of data into memory as we did in Exercise 1. The subset of data that we are using is pretty small, so the operation was relatively quick. If we wanted to use this naive approach with a larger AOI (say, the whole tile), we would want to consider chunking. \n",
    "\n",
    "In the [`rioxarray.open_rasterio()`]() call, we have the option of specifying `chunks`. Data chunking in Xarray (with Dask) is a way to break up large datasets into smaller, manageable pieces (\"chunks\") that can be processed lazily and in parallel. It’s essential when working with out-of-core data — data too big to fit into memory. \n",
    "\n",
    "This often leads to the age-old question: **How should I chunk my data?** See the Appendix for a walkthrough on how to calculate chunks based on the desired chunk size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss the data model here for a moment: we now have a 3D array of the shape `(year: 5, y: 2000, x: 4000)`. We have essentially stacked 5 years worth of 2D (x,y) array data into a data cube. TODO\n",
    "\n",
    "Now let's visualize our data cube!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_leafmap(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hvplot(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take this exercise a step further and explore how we can integrate Icechunk into this workflow for data versioning and management. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "em-workshop-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
